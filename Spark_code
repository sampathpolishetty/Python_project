data = [(1, "John", 1986),
  (2, "Ive", 1990),
  (4, "John",  1995)]

data2=[(1, "John", "Doe", 1986),
  (2, "IveNew", "Fish", 1990),
  (3, "San", "Simon", 1974)]
  
 
 
df1 = spark.createDataFrame(data=data, schema =columns)

df2 = spark.createDataFrame(data=data2, schema =columns_new)


df3=df2.withColumn('FullName',concat(df2.col('first'),df2.col('last'))).drop('first','last')

from pyspark.sql.functions import col, concat, lit

df3=df2.withColumn('FullName',concat(col('first'),col('last'))).drop('first','last')

def df_subtract(comp:DataFrame,main:DataFrame,key1,key2,composite_key):
     comp=comp.withColumn(composite_key,concat(col(key1),col(key2))).drop(key1,key2)
	   diff=comp.subtract(main)
	 return diff
	 
	 


------------------------------------------------

 def df_concat(sdf: DataFrame,kwargs):
        composite_key = kwargs["composite_key"]
		key1 = kwargs["s_composite_key"][0]
		key2 = kwargs["s_composite_key"][1]
 
        sdf = sdf.withColumn(
            composite_key, concat(col(key1), col(key2))
        ).drop(key1, key2)
        return sdf
	
	
sdf = df_concat(sdf,kwargs)


if len(kwargs["composite_key"]) == 1 and len(kwargs["composite_key"]) == 2:



	

